def S3(access_key=None, secret_key=None, region=None):
    """
    Create an S3 client.

    Args:
        access_key: AWS access key ID (defaults to AWS_ACCESS_KEY_ID env var)
        secret_key: AWS secret access key (defaults to AWS_SECRET_ACCESS_KEY env var)
        region: AWS region (defaults to AWS_REGION or us-east-1)

    Returns:
        An S3 client struct with methods for S3 operations.

    Example:
        s3 = S3()
        s3.upload("my-bucket", "path/to/file.txt", "hello world")
        data = s3.download("my-bucket", "path/to/file.txt")
        s3.delete("my-bucket", "path/to/file.txt")
    """
    ak = access_key if access_key else env("AWS_ACCESS_KEY_ID")
    sk = secret_key if secret_key else env("AWS_SECRET_ACCESS_KEY")
    rg = region if region else env("AWS_REGION")
    if not rg:
        rg = env("AWS_DEFAULT_REGION")
    if not rg:
        rg = "us-east-1"

    if not ak:
        fail("AWS access key not found. Set AWS_ACCESS_KEY_ID or provide access_key parameter")
    if not sk:
        fail("AWS secret key not found. Set AWS_SECRET_ACCESS_KEY or provide secret_key parameter")

    def _get_signing_key(date_stamp):
        k_date = hmac_sha256("AWS4" + sk, date_stamp)
        k_region = hmac_sha256(k_date, rg, key_hex=True)
        k_service = hmac_sha256(k_region, "s3", key_hex=True)
        k_signing = hmac_sha256(k_service, "aws4_request", key_hex=True)
        return k_signing

    def _url_encode(s, safe=""):
        result = ""
        for c in s:
            if c.isalnum() or c == "-" or c == "_" or c == "." or c == "~" or c in safe:
                result = result + c
            else:
                code = ord(c)
                hex_chars = "0123456789ABCDEF"
                result = result + "%" + hex_chars[code // 16] + hex_chars[code % 16]
        return result

    def _format_amz_date(timestamp):
        secs = int(timestamp)
        days_since_epoch = secs // 86400
        time_of_day = secs % 86400
        hours = time_of_day // 3600
        minutes = (time_of_day % 3600) // 60
        seconds = time_of_day % 60

        year = 1970
        days = days_since_epoch
        for _ in range(200):
            is_leap = (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)
            days_in_year = 366 if is_leap else 365
            if days < days_in_year:
                break
            days = days - days_in_year
            year = year + 1

        month = 1
        for _ in range(12):
            if month == 2:
                is_leap = (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)
                dim = 29 if is_leap else 28
            elif month in [4, 6, 9, 11]:
                dim = 30
            else:
                dim = 31
            if days < dim:
                break
            days = days - dim
            month = month + 1

        day = days + 1

        def pad2(n):
            return ("0" + str(n)) if n < 10 else str(n)

        amz_date = str(year) + pad2(month) + pad2(day) + "T" + pad2(hours) + pad2(minutes) + pad2(seconds) + "Z"
        date_stamp = str(year) + pad2(month) + pad2(day)
        return {"amz_date": amz_date, "date_stamp": date_stamp}

    def _sign_request(method, host, path, query_params, headers, payload):
        now = time()
        t = _format_amz_date(now)
        amz_date = t["amz_date"]
        date_stamp = t["date_stamp"]

        canonical_uri = path if path else "/"

        canonical_querystring = ""
        if query_params:
            sorted_keys = sorted(list(query_params.keys()))
            parts = []
            for key in sorted_keys:
                parts.append(_url_encode(key) + "=" + _url_encode(str(query_params[key])))
            canonical_querystring = "&".join(parts)

        headers_to_sign = {}
        headers_to_sign["host"] = host
        headers_to_sign["x-amz-date"] = amz_date
        headers_to_sign["x-amz-content-sha256"] = sha256(payload)

        for k in headers:
            headers_to_sign[k.lower()] = headers[k]

        sorted_header_keys = sorted(list(headers_to_sign.keys()))
        canonical_headers = ""
        signed_headers = ""
        for key in sorted_header_keys:
            canonical_headers = canonical_headers + key + ":" + headers_to_sign[key] + "\n"
            if signed_headers:
                signed_headers = signed_headers + ";" + key
            else:
                signed_headers = key

        payload_hash = sha256(payload)
        canonical_request = method + "\n" + canonical_uri + "\n" + canonical_querystring + "\n" + canonical_headers + "\n" + signed_headers + "\n" + payload_hash

        algorithm = "AWS4-HMAC-SHA256"
        credential_scope = date_stamp + "/" + rg + "/s3/aws4_request"
        string_to_sign = algorithm + "\n" + amz_date + "\n" + credential_scope + "\n" + sha256(canonical_request)

        signing_key = _get_signing_key(date_stamp)
        signature = hmac_sha256(signing_key, string_to_sign, key_hex=True)

        authorization_header = algorithm + " Credential=" + ak + "/" + credential_scope + ", SignedHeaders=" + signed_headers + ", Signature=" + signature

        result_headers = {}
        for k in headers:
            result_headers[k] = headers[k]
        result_headers["Host"] = host
        result_headers["X-Amz-Date"] = amz_date
        result_headers["X-Amz-Content-Sha256"] = payload_hash
        result_headers["Authorization"] = authorization_header

        return result_headers

    def _host(bucket):
        return bucket + ".s3." + rg + ".amazonaws.com"

    def _upload(bucket, key, data, content_type=None):
        if content_type == None:
            content_type = _guess_content_type(key)
        host = _host(bucket)
        path = "/" + key
        headers = {"Content-Type": content_type}
        signed_headers = _sign_request("PUT", host, path, None, headers, data)
        url = "https://" + host + path
        resp = http_request("PUT", url, headers=signed_headers, body=data)
        if resp.status < 200 or resp.status >= 300:
            fail("S3 upload failed: " + resp.body)
        return {"bucket": bucket, "key": key, "size": len(data)}

    def _upload_file(bucket, key, file, content_type=None):
        if content_type == None:
            content_type = _guess_content_type(file)
        data = read_file(file)
        return _upload(bucket, key, data, content_type)

    def _download(bucket, key):
        host = _host(bucket)
        path = "/" + key
        signed_headers = _sign_request("GET", host, path, None, {}, "")
        url = "https://" + host + path
        resp = http_request("GET", url, headers=signed_headers)
        if resp.status < 200 or resp.status >= 300:
            fail("S3 download failed: " + resp.body)
        return resp.body

    def _download_file(bucket, key, file):
        data = _download(bucket, key)
        write_file(file, data)
        return {"bucket": bucket, "key": key, "file": file, "size": len(data)}

    def _delete(bucket, *keys):
        host = _host(bucket)
        for key in keys:
            path = "/" + key
            signed_headers = _sign_request("DELETE", host, path, None, {}, "")
            url = "https://" + host + path
            resp = http_request("DELETE", url, headers=signed_headers)
            if resp.status < 200 or resp.status >= 300:
                fail("S3 delete failed: " + resp.body)
        return len(keys)

    def _exists(bucket, key):
        host = _host(bucket)
        path = "/" + key
        signed_headers = _sign_request("HEAD", host, path, None, {}, "")
        url = "https://" + host + path
        resp = http_request("HEAD", url, headers=signed_headers)
        return resp.status >= 200 and resp.status < 300

    def _list(bucket, prefix=None, delimiter=None, max_keys=None):
        host = _host(bucket)
        path = "/"
        query_params = {"list-type": "2"}
        if prefix:
            query_params["prefix"] = prefix
        if delimiter:
            query_params["delimiter"] = delimiter
        if max_keys:
            query_params["max-keys"] = str(max_keys)

        signed_headers = _sign_request("GET", host, path, query_params, {}, "")

        sorted_keys = sorted(list(query_params.keys()))
        parts = []
        for k in sorted_keys:
            parts.append(_url_encode(k) + "=" + _url_encode(str(query_params[k])))
        query_string = "&".join(parts)

        url = "https://" + host + path + "?" + query_string
        resp = http_request("GET", url, headers=signed_headers)

        if resp.status < 200 or resp.status >= 300:
            fail("S3 list failed: " + resp.body)

        return _parse_list_response(resp.body)

    def _parse_list_response(xml_body):
        objects = []
        pos = 0
        for _ in range(10000):
            start = xml_body.find("<Contents>", pos)
            if start == -1:
                break
            end = xml_body.find("</Contents>", start)
            if end == -1:
                break
            content = xml_body[start:end]
            obj = {}
            key_start = content.find("<Key>")
            key_end = content.find("</Key>")
            if key_start != -1 and key_end != -1:
                obj["key"] = content[key_start + 5:key_end]
            size_start = content.find("<Size>")
            size_end = content.find("</Size>")
            if size_start != -1 and size_end != -1:
                obj["size"] = int(content[size_start + 6:size_end])
            modified_start = content.find("<LastModified>")
            modified_end = content.find("</LastModified>")
            if modified_start != -1 and modified_end != -1:
                obj["last_modified"] = content[modified_start + 14:modified_end]
            objects.append(obj)
            pos = end + 11
        return objects

    def _copy(src_bucket, src_key, dest_bucket, dest_key):
        dest_host = _host(dest_bucket)
        path = "/" + dest_key
        copy_source = "/" + src_bucket + "/" + src_key
        headers = {"x-amz-copy-source": copy_source}
        signed_headers = _sign_request("PUT", dest_host, path, None, headers, "")
        url = "https://" + dest_host + path
        resp = http_request("PUT", url, headers=signed_headers)
        if resp.status < 200 or resp.status >= 300:
            fail("S3 copy failed: " + resp.body)
        return {"src_bucket": src_bucket, "src_key": src_key, "dest_bucket": dest_bucket, "dest_key": dest_key}

    def _metadata(bucket, key):
        host = _host(bucket)
        path = "/" + key
        signed_headers = _sign_request("HEAD", host, path, None, {}, "")
        url = "https://" + host + path
        resp = http_request("HEAD", url, headers=signed_headers)
        if resp.status < 200 or resp.status >= 300:
            fail("S3 metadata failed: " + resp.body)
        data = {"bucket": bucket, "key": key}
        if "content-length" in resp.headers:
            data["size"] = int(resp.headers["content-length"])
        if "content-type" in resp.headers:
            data["content_type"] = resp.headers["content-type"]
        if "last-modified" in resp.headers:
            data["last_modified"] = resp.headers["last-modified"]
        if "etag" in resp.headers:
            data["etag"] = resp.headers["etag"]
        return data

    def _guess_content_type(path):
        ext = ""
        if "." in path:
            parts = path.split(".")
            ext = parts[len(parts) - 1].lower()
        content_types = {
            "json": "application/json", "txt": "text/plain", "html": "text/html",
            "css": "text/css", "js": "application/javascript", "xml": "application/xml",
            "jpg": "image/jpeg", "jpeg": "image/jpeg", "png": "image/png",
            "gif": "image/gif", "svg": "image/svg+xml", "pdf": "application/pdf",
            "zip": "application/zip", "gz": "application/gzip", "csv": "text/csv",
            "md": "text/markdown", "yaml": "application/yaml", "yml": "application/yaml"
        }
        if ext in content_types:
            return content_types[ext]
        return "application/octet-stream"

    return {
        "upload": _upload,
        "upload_file": _upload_file,
        "download": _download,
        "download_file": _download_file,
        "delete": _delete,
        "exists": _exists,
        "list": _list,
        "copy": _copy,
        "metadata": _metadata,
        "region": rg,
    }
